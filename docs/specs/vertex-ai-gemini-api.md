# Vertex AI Gemini API Specification

## Overview

The Vertex AI Gemini API provides access to Google's Gemini family of large language models through the `generateContent` and `streamGenerateContent` methods. The API supports multimodal inputs (text, images, audio, video, PDF), function calling, streaming responses, and advanced safety controls.

**Endpoint**: 
- **Generate Content**: `POST https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/publishers/google/models/{model_id}:generateContent`
- **Stream Generate Content**: `POST https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/publishers/google/models/{model_id}:streamGenerateContent`

> [!NOTE]
> All Gemini models in Vertex AI support content generation with multimodal capabilities.

## Authentication

Requests require proper Google Cloud authentication:
- **Service Account**: Use service account credentials with appropriate IAM permissions
- **OAuth 2.0**: For user-based authentication
- **API Key**: Not supported - use Google Cloud authentication methods

Required IAM permission: `aiplatform.endpoints.predict`

## Request Format

### Basic Structure

```json
{
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "How does AI work?"
        }
      ]
    }
  ],
  "generationConfig": {
    "temperature": 1.0,
    "topP": 0.95,
    "maxOutputTokens": 8192
  }
}
```

### Request Parameters

#### Required Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `contents` | array | The content of the current conversation with the model. For single-turn queries, this is a single instance. For multi-turn queries, this is a repeated field containing conversation history and the latest request. |

#### Optional Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `systemInstruction` | object | Instructions for the model to steer it toward better performance (e.g., "Answer as concisely as possible"). Available for `gemini-2.0-flash` and `gemini-2.0-flash-lite`. |
| `generationConfig` | object | Generation configuration settings that control randomness, length, and output structure. |
| `safetySettings` | array | Per-request settings for blocking unsafe content. Enforced on `GenerateContentResponse.candidates`. |
| `tools` | array | A piece of code that enables the system to interact with external systems to perform actions outside the model's knowledge and scope. See Function Calling. |
| `toolConfig` | object | Configuration for tool usage. |
| `cachedContent` | string | The name of cached content used as context (format: `projects/{project}/locations/{location}/cachedContents/{cachedContent}`). |
| `labels` | object | Metadata in key-value pairs for billing and reporting. |

### Contents Structure

The `contents` parameter contains the conversation history and current prompt.

#### Content Object

| Field | Type | Description |
|-------|------|-------------|
| `role` | string | The identity of the entity that creates the message. Values: `user` (real person/user-generated) or `model` (generated by the model, used for multi-turn conversations). |
| `parts` | array | List of ordered parts that make up a single message. Different parts may have different IANA MIME types. |

#### Part Object

Each part can contain one of the following data types:

| Field | Type | Description |
|-------|------|-------------|
| `text` | string | A text prompt or code snippet. |
| `inlineData` | object | Inline data in raw bytes. Contains `mimeType` and base64-encoded `data`. Maximum size: 20MB. |
| `fileData` | object | Data stored in a file. Contains `mimeType` and `fileUri` (Cloud Storage URI, HTTP URL, or YouTube video URL). |
| `functionCall` | object | A predicted function call from the model containing the function name and parameters. |
| `functionResponse` | object | The result of a function call containing the function name and output. |
| `videoMetadata` | object | Metadata for video input including `startOffset`, `endOffset`, and `fps`. |
| `mediaResolution` | enum | Controls how input media is processed. Values: `HIGH`, `MEDIUM`, `LOW`. Overrides `generationConfig.mediaResolution` if specified. |

#### Supported MIME Types

**Images**:
- `image/png`
- `image/jpeg`
- `image/webp`

**Audio**:
- `audio/mpeg`
- `audio/mp3`
- `audio/wav`

**Video**:
- `video/mov`
- `video/mpeg`
- `video/mp4`
- `video/mpg`
- `video/avi`
- `video/wmv`
- `video/mpegps`
- `video/flv`

**Documents**:
- `application/pdf`
- `text/plain`

> [!IMPORTANT]
> For `gemini-2.0-flash-lite` and `gemini-2.0-flash`:
> - Maximum audio file length: 8.4 hours
> - Maximum video file length (without audio): 1 hour
> - Up to 3000 images can be specified using `inlineData`
> - Cloud Storage file size limit: 2 GB
> - HTTP URL file size limit: 15 MB

### System Instruction

```json
{
  "systemInstruction": {
    "role": "string",
    "parts": [
      {
        "text": "You are a helpful assistant that provides concise answers."
      }
    ]
  }
}
```

> [!NOTE]
> The `role` field of `systemInstruction` is ignored and doesn't affect model performance.

### Generation Config

The `generationConfig` object controls how the model generates content:

| Parameter | Type | Range | Default | Description |
|-----------|------|-------|---------|-------------|
| `temperature` | float | 0.0 - 2.0 | 1.0 | Controls randomness in token selection. Lower values (e.g., 0) are more deterministic; higher values are more creative. |
| `topP` | float | 0.0 - 1.0 | 0.95 | Nucleus sampling threshold. Tokens are selected from most to least probable until their cumulative probability equals `topP`. |
| `topK` | integer | - | - | Number of top tokens to consider during sampling. |
| `candidateCount` | integer | 1 - 8 | 1 | Number of response variations to return. Only works with `generateContent` (not streaming). **Preview feature** |
| `maxOutputTokens` | integer | - | varies | Maximum number of tokens that can be generated in the response. A token is approximately four characters. |
| `stopSequences` | array | - | - | List of strings that tell the model to stop generating text if encountered. Maximum 5 items. Case-sensitive. |
| `presencePenalty` | float | -2.0 to <2.0 | - | Penalizes tokens that already appear in the generated text, increasing diversity. |
| `frequencyPenalty` | float | -2.0 to <2.0 | - | Penalizes tokens that repeatedly appear, decreasing repetition. |
| `responseMimeType` | string enum | - | `text/plain` | Output response MIME type. Options: `text/plain`, `application/json`, `text/x.enum` |
| `responseSchema` | object | - | - | Schema that generated candidate text must follow. Requires `responseMimeType` other than `text/plain`. |
| `seed` | integer | - | - | For deterministic sampling to ensure reproducible results. |
| `responseLogprobs` | boolean | - | false | Whether to return log probabilities of output tokens. |
| `logprobs` | integer | - | - | Number of log probabilities to return. |
| `audioTimestamp` | boolean | - | - | Include audio timestamps in the response. |
| `thinkingConfig` | object | - | - | Configuration for thinking budget and level. |
| `mediaResolution` | enum | - | - | Controls how input media is processed globally. Values: `HIGH`, `MEDIUM`, `LOW`. |

> [!TIP]
> Best practice: Modify either `temperature` OR `topP`, not both simultaneously.

### Safety Settings

Configure content filtering and safety controls:

```json
{
  "safetySettings": [
    {
      "category": "HARM_CATEGORY_HATE_SPEECH",
      "threshold": "BLOCK_MEDIUM_AND_ABOVE",
      "method": "PROBABILITY"
    }
  ]
}
```

#### Safety Categories

- `HARM_CATEGORY_SEXUALLY_EXPLICIT`
- `HARM_CATEGORY_HATE_SPEECH`
- `HARM_CATEGORY_HARASSMENT`
- `HARM_CATEGORY_DANGEROUS_CONTENT`

#### Harm Block Thresholds

- `BLOCK_LOW_AND_ABOVE` - Block low threshold and higher (block more)
- `BLOCK_MEDIUM_AND_ABOVE` - Block medium threshold and higher
- `BLOCK_ONLY_HIGH` - Block only high threshold (block less)
- `BLOCK_NONE` - Block none
- `OFF` - Switches off safety if all categories are turned OFF

#### Harm Block Methods

- `PROBABILITY` - Uses the probability score (default)
- `SEVERITY` - Uses both probability and severity scores

### Function Calling

Define functions/tools the model can call:

```json
{
  "tools": [
    {
      "functionDeclarations": [
        {
          "name": "get_weather",
          "description": "Get the current weather in a location",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "City name"
              },
              "unit": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"]
              }
            },
            "required": ["location"]
          }
        }
      ]
    }
  ]
}
```

Parameters follow the [OpenAPI 3.0 Schema](https://spec.openapis.org/oas/v3.0.3#schema) specification.

## Response Format

### Standard Response (Non-Streaming)

```json
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "AI works through a combination of data, algorithms, and computational power..."
          }
        ]
      },
      "finishReason": "FINISH_REASON_STOP",
      "safetyRatings": [
        {
          "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
          "probability": "NEGLIGIBLE",
          "blocked": false
        }
      ],
      "citationMetadata": {
        "citations": []
      }
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 10,
    "candidatesTokenCount": 150,
    "totalTokenCount": 160
  },
  "modelVersion": "gemini-2.0-flash-001"
}
```

### Response Fields

| Field | Type | Description |
|-------|------|-------------|
| `candidates` | array | Array of generated response candidates. |
| `usageMetadata` | object | Token usage information. |
| `modelVersion` | string | The specific model version used (e.g., `gemini-2.0-flash-001`). |

#### Candidate Object

| Field | Type | Description |
|-------|------|-------------|
| `content` | object | The generated content with `parts` array. |
| `finishReason` | enum | Why the model stopped generating. |
| `safetyRatings` | array | Safety ratings for the generated content. |
| `citationMetadata` | object | Citation information for the response. |
| `avgLogprobs` | double | Average log probabilities. |
| `logprobsResult` | object | Detailed log probabilities result. |

#### Finish Reasons

| Value | Description |
|-------|-------------|
| `FINISH_REASON_STOP` | Natural stop point of the model or provided stop sequence. |
| `FINISH_REASON_MAX_TOKENS` | Maximum number of tokens reached. |
| `FINISH_REASON_SAFETY` | Token generation stopped due to safety reasons. `Candidate.content` is empty. |
| `FINISH_REASON_RECITATION` | Stopped because response was flagged for unauthorized citations. |
| `FINISH_REASON_BLOCKLIST` | Stopped because response includes blocked terms. |
| `FINISH_REASON_PROHIBITED_CONTENT` | Stopped due to prohibited content (e.g., CSAM). |
| `FINISH_REASON_IMAGE_PROHIBITED_CONTENT` | Image in prompt was flagged for prohibited content. |
| `FINISH_REASON_NO_IMAGE` | Image was expected but none provided. |
| `FINISH_REASON_SPII` | Stopped due to sensitive personally identifiable information. |
| `FINISH_REASON_MALFORMED_FUNCTION_CALL` | Malformed and unparsable function call. |
| `FINISH_REASON_OTHER` | All other reasons. |
| `FINISH_REASON_UNSPECIFIED` | Unspecified finish reason. |

#### Safety Rating Object

| Field | Type | Description |
|-------|------|-------------|
| `category` | enum | The harm category (same values as safety settings). |
| `probability` | enum | Probability of harm: `NEGLIGIBLE`, `LOW`, `MEDIUM`, `HIGH`, `HARM_PROBABILITY_UNSPECIFIED` |
| `blocked` | boolean | Whether the content was blocked. |

#### Citation Metadata

| Field | Type | Description |
|-------|------|-------------|
| `startIndex` | integer | Start index of the citation in the content. |
| `endIndex` | integer | End index of the citation in the content. |
| `uri` | string | URL of the cited source. |
| `title` | string | Title of the cited source. |
| `license` | string | License of the cited source. |
| `publicationDate` | object | Publication date (format: `YYYY`, `YYYY-MM`, or `YYYY-MM-DD`). |

#### Usage Metadata

| Field | Type | Description |
|-------|------|-------------|
| `promptTokenCount` | integer | Number of tokens in the prompt. |
| `candidatesTokenCount` | integer | Number of tokens in all generated candidates. |
| `totalTokenCount` | integer | Total tokens used (prompt + candidates). |

### Function Call Response

When the model calls a function:

```json
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "functionCall": {
              "name": "get_weather",
              "args": {
                "location": "San Francisco",
                "unit": "celsius"
              }
            }
          }
        ]
      },
      "finishReason": "FINISH_REASON_STOP"
    }
  ]
}
```

To provide the function result, send it back in a subsequent request:

```json
{
  "contents": [
    {
      "role": "model",
      "parts": [
        {
          "functionCall": {
            "name": "get_weather",
            "args": {
              "location": "San Francisco",
              "unit": "celsius"
            }
          }
        }
      ]
    },
    {
      "role": "user",
      "parts": [
        {
          "functionResponse": {
            "name": "get_weather",
            "response": {
              "temperature": 18,
              "condition": "sunny"
            }
          }
        }
      ]
    }
  ]
}
```

## Streaming Response

When using `streamGenerateContent`, the response is sent as a stream of chunks.

> [!NOTE]
> `candidateCount` > 1 is not supported with streaming.

Each chunk follows the same structure as the standard response but may contain partial content. The stream ends when you receive a chunk with a `finishReason`.

## Supported Models

Common Gemini models available in Vertex AI:

- `gemini-2.0-flash` - Latest multimodal flagship model
- `gemini-2.0-flash-lite` - Lighter, faster version
- `gemini-2.5-flash` - Enhanced version (preview)
- `gemini-1.5-pro` - High-capability model
- `gemini-1.5-flash` - Previous generation fast model

> [!IMPORTANT]
> Model availability varies by region. Check the [Google models page](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) for current model specifications and availability.

## Example Requests

### Simple Text Generation

```bash
curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/gemini-2.0-flash:generateContent \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Explain how AI works in simple terms."
          }
        ]
      }
    ]
  }'
```

### Multimodal Request (Text + Image)

```bash
curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/gemini-2.0-flash:generateContent \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "What is in this image?"
          },
          {
            "fileData": {
              "mimeType": "image/png",
              "fileUri": "gs://my-bucket/image.png"
            }
          }
        ]
      }
    ]
  }'
```

### Multi-turn Conversation

```bash
curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/gemini-2.0-flash:generateContent \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [{"text": "What is the capital of France?"}]
      },
      {
        "role": "model",
        "parts": [{"text": "The capital of France is Paris."}]
      },
      {
        "role": "user",
        "parts": [{"text": "What is its population?"}]
      }
    ]
  }'
```

### Function Calling

```bash
curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/gemini-2.0-flash:generateContent \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [{"text": "What is the weather in Boston?"}]
      }
    ],
    "tools": [
      {
        "functionDeclarations": [
          {
            "name": "get_weather",
            "description": "Get current weather for a location",
            "parameters": {
              "type": "object",
              "properties": {
                "location": {
                  "type": "string",
                  "description": "City name"
                }
              },
              "required": ["location"]
            }
          }
        ]
      }
    ]
  }'
```

### Streaming Request

```bash
curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/gemini-2.0-flash:streamGenerateContent \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [{"text": "Tell me a story about AI"}]
      }
    ]
  }'
```

### JSON Output Mode

```bash
curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/gemini-2.0-flash:generateContent \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [{"text": "List three colors in JSON format"}]
      }
    ],
    "generationConfig": {
      "responseMimeType": "application/json",
      "responseSchema": {
        "type": "object",
        "properties": {
          "colors": {
            "type": "array",
            "items": {
              "type": "string"
            }
          }
        }
      }
    }
  }'
```

## Best Practices

1. **Temperature vs TopP**: Modify only one at a time for best results
2. **Token Management**: Monitor `usageMetadata` and set `maxOutputTokens` to control costs
3. **Multimodal Inputs**: Use Cloud Storage URIs for large files to avoid base64 encoding overhead
4. **Function Calling**: Always validate function arguments before execution
5. **Safety Settings**: Configure appropriate thresholds based on your use case
6. **Streaming**: Use streaming for better user experience in interactive applications
7. **System Instructions**: Leverage system instructions to set consistent behavior
8. **Error Handling**: Implement retry logic with exponential backoff for transient errors
9. **Media Resolution**: Use `LOW` resolution for longer videos to stay within token limits

## Error Handling

Errors follow standard Google Cloud error responses:

```json
{
  "error": {
    "code": 400,
    "message": "Invalid request",
    "status": "INVALID_ARGUMENT",
    "details": [...]
  }
}
```

Common error codes:
- `400` - Invalid request (bad parameters)
- `401` - Authentication failed
- `403` - Permission denied
- `404` - Resource not found
- `429` - Rate limit exceeded
- `500` - Internal server error
- `503` - Service unavailable

## Rate Limits

Rate limits vary by:
- Project quota
- Model used
- Region

Monitor your quota usage in the Google Cloud Console. Implement exponential backoff when receiving 429 errors.

## References

- [Vertex AI Gemini API Documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini)
- [Multimodal Prompts Guide](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-multimodal-prompts)
- [Function Calling Guide](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/function-calling)
- [Control Generated Output](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output)
- [Supported Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models)
